{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1dcc050e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a15b8777",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WineDataset(Dataset):\n",
    "    def __init__(self, path):\n",
    "        # data loading\n",
    "        xy = np.loadtxt(path, delimiter=',', dtype=np.float32, skiprows=1)\n",
    "        self.X = torch.from_numpy(xy[:, 0:-1])\n",
    "        self.y = torch.from_numpy(xy[:, -1]).view([-1,1])\n",
    "        self.n_samples = xy.shape[0]\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        # dataset[0]\n",
    "        return self.X[index], self.y[index]\n",
    "        \n",
    "    def __len__(self):\n",
    "        # len(dataset)\n",
    "        return self.n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6cf9c394",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = WineDataset('lecture_07_train.csv')\n",
    "ds_test = WineDataset('lecture_07_test.csv')\n",
    "train_dataloader = DataLoader(dataset=ds_train, batch_size=4, shuffle=True)\n",
    "test_dataloader = DataLoader(dataset=ds_test, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "544ac78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(11, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        pred = self.linear_relu_stack(x)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2626a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    print(f\"Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78bb7ed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 75.259491  [    4/ 3918]\n",
      "loss: 6.828148  [  404/ 3918]\n",
      "loss: 1.104428  [  804/ 3918]\n",
      "loss: 2.881206  [ 1204/ 3918]\n",
      "loss: 2.337165  [ 1604/ 3918]\n",
      "loss: 1.189135  [ 2004/ 3918]\n",
      "loss: 1.564875  [ 2404/ 3918]\n",
      "loss: 2.931337  [ 2804/ 3918]\n",
      "loss: 1.696848  [ 3204/ 3918]\n",
      "loss: 0.911774  [ 3604/ 3918]\n",
      "Avg loss: 1.697283 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 1.013132  [    4/ 3918]\n",
      "loss: 1.035671  [  404/ 3918]\n",
      "loss: 0.131800  [  804/ 3918]\n",
      "loss: 0.352381  [ 1204/ 3918]\n",
      "loss: 0.467473  [ 1604/ 3918]\n",
      "loss: 0.539987  [ 2004/ 3918]\n",
      "loss: 1.776217  [ 2404/ 3918]\n",
      "loss: 0.901376  [ 2804/ 3918]\n",
      "loss: 2.305511  [ 3204/ 3918]\n",
      "loss: 1.438245  [ 3604/ 3918]\n",
      "Avg loss: 0.859446 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.484519  [    4/ 3918]\n",
      "loss: 0.325553  [  404/ 3918]\n",
      "loss: 0.420671  [  804/ 3918]\n",
      "loss: 0.126817  [ 1204/ 3918]\n",
      "loss: 0.234268  [ 1604/ 3918]\n",
      "loss: 0.964683  [ 2004/ 3918]\n",
      "loss: 0.831622  [ 2404/ 3918]\n",
      "loss: 0.182909  [ 2804/ 3918]\n",
      "loss: 0.716455  [ 3204/ 3918]\n",
      "loss: 0.575939  [ 3604/ 3918]\n",
      "Avg loss: 1.899907 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.821427  [    4/ 3918]\n",
      "loss: 1.123687  [  404/ 3918]\n",
      "loss: 0.127650  [  804/ 3918]\n",
      "loss: 0.130917  [ 1204/ 3918]\n",
      "loss: 0.240724  [ 1604/ 3918]\n",
      "loss: 0.527651  [ 2004/ 3918]\n",
      "loss: 0.612205  [ 2404/ 3918]\n",
      "loss: 1.032206  [ 2804/ 3918]\n",
      "loss: 0.740767  [ 3204/ 3918]\n",
      "loss: 0.308450  [ 3604/ 3918]\n",
      "Avg loss: 0.689821 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.170602  [    4/ 3918]\n",
      "loss: 0.055403  [  404/ 3918]\n",
      "loss: 2.837231  [  804/ 3918]\n",
      "loss: 2.729293  [ 1204/ 3918]\n",
      "loss: 1.353314  [ 1604/ 3918]\n",
      "loss: 1.437365  [ 2004/ 3918]\n",
      "loss: 0.432278  [ 2404/ 3918]\n",
      "loss: 0.494139  [ 2804/ 3918]\n",
      "loss: 1.156352  [ 3204/ 3918]\n",
      "loss: 0.976035  [ 3604/ 3918]\n",
      "Avg loss: 1.028458 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 1.070723  [    4/ 3918]\n",
      "loss: 0.831745  [  404/ 3918]\n",
      "loss: 0.258224  [  804/ 3918]\n",
      "loss: 1.965124  [ 1204/ 3918]\n",
      "loss: 0.782986  [ 1604/ 3918]\n",
      "loss: 1.676847  [ 2004/ 3918]\n",
      "loss: 1.130572  [ 2404/ 3918]\n",
      "loss: 1.440178  [ 2804/ 3918]\n",
      "loss: 0.090379  [ 3204/ 3918]\n",
      "loss: 0.061804  [ 3604/ 3918]\n",
      "Avg loss: 0.776649 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.861421  [    4/ 3918]\n",
      "loss: 0.322523  [  404/ 3918]\n",
      "loss: 0.437092  [  804/ 3918]\n",
      "loss: 0.962689  [ 1204/ 3918]\n",
      "loss: 2.837414  [ 1604/ 3918]\n",
      "loss: 0.111268  [ 2004/ 3918]\n",
      "loss: 1.332290  [ 2404/ 3918]\n",
      "loss: 0.355642  [ 2804/ 3918]\n",
      "loss: 0.400203  [ 3204/ 3918]\n",
      "loss: 0.730136  [ 3604/ 3918]\n",
      "Avg loss: 0.653004 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.611190  [    4/ 3918]\n",
      "loss: 0.083468  [  404/ 3918]\n",
      "loss: 0.328412  [  804/ 3918]\n",
      "loss: 0.582824  [ 1204/ 3918]\n",
      "loss: 0.360056  [ 1604/ 3918]\n",
      "loss: 0.974758  [ 2004/ 3918]\n",
      "loss: 0.778697  [ 2404/ 3918]\n",
      "loss: 0.662699  [ 2804/ 3918]\n",
      "loss: 0.067841  [ 3204/ 3918]\n",
      "loss: 1.983676  [ 3604/ 3918]\n",
      "Avg loss: 0.766901 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 1.580697  [    4/ 3918]\n",
      "loss: 0.306641  [  404/ 3918]\n",
      "loss: 2.022767  [  804/ 3918]\n",
      "loss: 0.907516  [ 1204/ 3918]\n",
      "loss: 0.370550  [ 1604/ 3918]\n",
      "loss: 0.288393  [ 2004/ 3918]\n",
      "loss: 0.321723  [ 2404/ 3918]\n",
      "loss: 0.272870  [ 2804/ 3918]\n",
      "loss: 0.366841  [ 3204/ 3918]\n",
      "loss: 1.231133  [ 3604/ 3918]\n",
      "Avg loss: 0.802624 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 1.636654  [    4/ 3918]\n",
      "loss: 1.674427  [  404/ 3918]\n",
      "loss: 0.405355  [  804/ 3918]\n",
      "loss: 0.036093  [ 1204/ 3918]\n",
      "loss: 0.131548  [ 1604/ 3918]\n",
      "loss: 0.740507  [ 2004/ 3918]\n",
      "loss: 0.196809  [ 2404/ 3918]\n",
      "loss: 1.594635  [ 2804/ 3918]\n",
      "loss: 1.207965  [ 3204/ 3918]\n",
      "loss: 1.152388  [ 3604/ 3918]\n",
      "Avg loss: 0.676846 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.0001\n",
    "model = NeuralNetwork()\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
